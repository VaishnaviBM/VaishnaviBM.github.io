<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vaishnavi B Mohan</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">

</head>

<body>
    <header>
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#research">Research Demos</a></li>
                <li><a href="#experience">Professional and Research Experience</a></li>
                <li><a href="#talks">Talks and Conferences</a></li>
                <li><a href="#cv">Curriculum Vitae</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
        </nav>
    </header>


    <main>
        <section id="home">
            <div class="profile">
                <img src="images/profile.jpg" alt="Vaishnavi B Mohan" class="profile-image">
                <h1>Vaishnavi B Mohan</h1>
                
                <div class="social-icons">
                    <a href="mailto:vaishnavibm95@gmail.com" target="_blank" class="contact-icon">
                        <i class="fas fa-envelope"></i> vaishnavibm95@gmail.com
                    </a>
                    <a href="mailto:vbmohan@uw.edu" target="_blank" class="contact-icon">
                        <i class="fas fa-envelope"></i> vbmohan@uw.edu
                    </a>
                    <a href="https://www.linkedin.com/in/vaishnavi-b-mohan1995/" target="_blank" class="linkedin-icon">
                        <i class="fab fa-linkedin"></i> LinkedIn
                    </a>
                    <a href="https://github.com/Vaishnavi-B-Mohan" target="_blank" class="github-icon">
                        <i class="fab fa-github"></i> GitHub
                    </a>
                    <a href="pdfs/Resume_VaishnaviBMohan.pdf" target="_blank" class="resume-link">
                        <i class="fas fa-file-alt"></i> Resume
                    </a>
                    <a href="pdfs/CV_VaishnaviBMohan.pdf" target="_blank">
                        <i class="fas fa-file-pdf"></i> CV
                    </a>
                </div>
 

                <p>Hi, I am Vaishnavi, a PhD student in Vision Neuroscience and Perception at the University of Washington, Seattle advised by <a href="https://psych.uw.edu/people/3572" target="_blank">Prof. Geoffrey Boynton </a> and <a href="https://psych.uw.edu/people/2400" target="_blank">Prof. Ione Fine.</a> I hold a B.Tech in Electrical and Electronics Engineering from  <a href="https://www.nitk.ac.in/ " target="_blank">National Institute of Technology Karnataka </a> and a Masters in Electrical Engineering from <a href="https://minghsiehece.usc.edu/academics/ms/" target="_blank">University of Southern California</a> where I specialized in signal processing and machine learning. Prior to my PhD, I spent about four years at <a href="https://www.intel.com/content/www/us/en/homepage.html" target="_blank">Intel Corporation, Hillsboro, OR </a> as a Software Development Engineer where I developed a strong background in computational methods and programming with applications to computer vision and large vision models.  </p>
            
                <p> Working with deep learning models which guzzle compute (and thereby, power) during inference, and data during training, I was always fascinated by human vision and how the brain learns to efficiently process such a rich array of information coming in through our eyes. Therefore, I began my PhD in <a href="https://courses.washington.edu/viscog/" target="_blank">Vision+Cognition Group</a> at UW. My research revolves around visual perception in the context of sight recovery technologies for late-onset of blindness. Late-onset of blindness is usually caused due to genetic mutations and can be devastating for people to lose their sight. Hence, there are multiple emerging technologies which are trying to make the visual system respond to light again in such individuals. However, the resulting perception can be highly distorted and different from neurotypical vision. My research is all about understanding how this neural response translates to perception: a simulation framework to predict the visual perceptual experience of these technologies to inform systematic improvement in design methodologies. </p>
            
                <p> Broadly, I develop perception models using psychophysics experiments, computational methods and information-theoretical approaches. I also develop theoretical mathematical models to simulate the neural responses for optogenetics-mediated vision. </p>


        </section>

        <section id="education">
            <h2>Education</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-date">Now </div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Ph.D in Psychology (Ongoing):  <em> Sept 2022 - Present </em> </h3>
                            <p class="place">Vision+Cognition Group, University of Washington, Seattle, WA</p>
                            <p class="place">Research area: Visual perception, Computational modeling, Visual Prostheses, Blindness, Retina</p>
                        </div>
                    </div>
                </div>
               

                <div class="timeline-item">
                    <div class="timeline-date">2018 </div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Master of Science: Electrical Engineering (MS Honors Fellow):  <em>Aug 2017 - Dec 2018 </em> </h3>
                            <p class="place">University of Southern California, Los Angeles, CA</p>
                            <p class="place">Research areas: Signal Processing, Machine Learning, Neurotechnology Design, Deep Learning</p>
                        </div>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-date">2017 </div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Bachelor of Technology: Electrical and Electronics Engineering:  <em>Aug 2013 - May 2017</em> </h3>
                            <p class="place">National Institute of Technology Karnataka, Surathkal, India</p>
                            <p class="place">Study areas: Signal Processing, Machine Learning, Electrical Systems, Circuit Theory, Control Theory</p>
                        </div>
                    </div>
                </div>
                </div>
            </div>
        </section>

        <section id="research">
                <h2>Research Demos</h2>
            
            <div class="research-grid">

                <!-- Tile  eyenchip -->
                <div class="research-tile">
                    <a href="pdfs/eyenthechip.pdf" target="_blank" id="uw_eyenchip">
                        <div class="tile-content">
                            <img src="thumbnails/eyenthechip.jpg" alt="Poster 1" class="tile-image" width="300" height="200">
                        </div>
                    </a>
                    <div class="tile-description">
                        <h3>Understanding Perceptual Outcomes of Optogenetic Vision</h3>
                        <p>
                            Here's a sneakpeak into my poster at The Eye and the Chip 13th World Research Congress on Artificial
                            Vision held in Michigan in October 2023
                        </p>
                    </div>
                </div>

                <!-- Tile uw_hhsim -->
                <div class="research-tile">
                    <video controls width="100%" height="auto">
                        <source src="videos/hh_currentClamp.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="tile-description">
                        <h3>Simulation of a Neuron Firing Action potential</h3>
                        <p>A short simulation of a neuron firing based on the Nobel Prize work of Hodgkin and Huxley(1952) in current clamp mode <a href="https://doi.org/10.1113/jphysiol.1952.sp004764" target="_blank">OG Paper</a>. The neuron fires as the external current input increases</p>
                    </div>
                </div>

                <!-- Tile intel_sf -->
                <div class="research-tile" id="intel_sf">
                    <a href="https://youtu.be/I37acV-Wzgw?feature=shared" target="_blank">
                        <div class="tile-content">
                            <iframe width="100%" height="180" src="https://www.youtube.com/embed/I37acV-Wzgw?si=tWRhTsvZ_YMF-YGP" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </div>
                    </a>
                    <div class="tile-description">
                        <h3>Intel Smart Framing PoC</h3>
                        <p>Now integrated into Microsoft Windows 11 OS, I developed the AI effects of auto-framing and background effects PoC based on deep learning models which segment parts of the camera stream for the Intel PoC.</p>
                    </div>
                </div>

            <!-- Tile usc_bci -->
            <div class="research-tile">
                <a href="Attention-BCI.pdf" target="_blank" id="usc_bci">
                    <div class="tile-content">
                        <img src="thumbnails/usc_bci.png" alt="Poster 1" class="tile-image" width="300" height="200">
                    </div>
                </a>
                <div class="tile-description">
                    <h3>Gaze-independent BCI based on Coverts Shifts in Attention</h3>
                    <p>
                        Here's a sneakpeak into my Masters thesis on BCIs based on covert attention which do not require users to stare at a screen, unlike P300 Spellers!
                         </p>
                </div>
            </div>
            <!-- Tile intel_hsb -->
            <div class="research-tile" id="intel_hsb">
                <a href="https://youtu.be/X35S3NSOHq0?feature=shared" target="_blank">
                    <div class="tile-content">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/X35S3NSOHq0?si=3LMI4PPnuydfpG31" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    </div>
                </a>
                <div class="tile-description">
                    <h3>PoC for Intel Horseshoe Bend - Foldable Display Laptops</h3>
                    <p>Now in production by HP, Asus, Lenovo, I designed and developed 310+ integrated test suite for end to end testing of the client platform design for the Intel PoC.</p>
                </div>
            </div>


            <!-- Tile uoft_bci -->
            <div class="research-tile">
                <a href="pdfs/uoft_bcis.pdf" target="_blank" id="uoft_bci">
                    <div class="tile-content">
                        <img src="thumbnails/uoft_bcis.jpg" alt="Poster 1" class="tile-image" width="300" height="200">
                    </div>
                </a>
                <div class="tile-description">
                    <h3>Optimal BCI based on Motor Imagery</h3>
                    <p>
                        Brain signals associated with actual movement of hands is similar to those of imagined hand movement. At UofT, I built a BCI based on EEG data from sensorimotor area to move a wheelchair based on this imagined hand movement.
                    </p>
                </div>
            </div>

        </div>
        </section>

        <section id="experience">
            <h2>Professional and Research Experience</h2>
            <div class="timeline">
                <div class="timeline-item">
                    <div class="timeline-date">2022 </div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>PhD Researcher</h3>
                            <p class="place">Vision+Cognition Group, University of Washington</p>
                        </div>
                        <ul class="timeline-bullets">
                            <li><strong>p2p_opto:</strong> Overarching virtual prototyping framework to predict perceptual outcomes of sight recovery technologies </li>
                            <li><strong>Spatiotemporal models:</strong> Developing mathematical models to simulate neural responses of optogenetic-mediated visual activation to light stimuli</li>
                            <li><strong>Psychophysics:</strong> Designing experiments based on psychophysics to measure contrast sensitivity and other metrics of visual acuity and color perception</li>
                            <li><strong>Predictive models:</strong> Building models to predict perceptual outcomes integrating neural and behavioral data for sight restoration technologies </li>
                            <a href="#uw_eyenchip" class="link-to-research">
                                See related project
                            </a>
                        </ul>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">2019</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Software Development Engineer</h3>
                            <p class="place">Intel Corporation, Hillsboro, OR</p>
                        </div>
                        <ul class="timeline-bullets">
                            <li><strong>PoC: Microsoft Windows 11 Studio Effects:</strong>  
                                Built low-power, low-latency AI applications for Intel edge platforms. Created the AI-based automatic framing and background effects PoC, now integrated into Microsoft Windows 11 OS.
                                <a href="#intel_sf" class="link-to-research">
                                    See related project
                                </a>
                            </li>

                            <li><strong>PoC: Intel Horseshoe Bend Foldable Display Laptops:</strong> 
                                Led development of a comprehensive test suite of 310+ tests for Intel’s foldable display laptop PoC (Horseshoe Bend) now in production with HP, Lenovo and Asus.
                                <a href="#intel_hsb" class="link-to-research">
                                    See related project
                                </a>
                            </li>
                            <li><strong>AI Model Performance Evaluation Tool: </strong>Built a computer vision tracking pipeline for a smart home visual analytics solution. Designed and developed an automated framework to evaluate KPIs for computer vision applications</li>
                        </ul>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">2018</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>AI Solution Software Graduate Intern</h3>
                            <p class="place">Intel Corporation, Hillsboro, OR</p>
                        </div>
                        <ul class="timeline-bullets">
                            <p><strong>Virtual Yoga Coach</strong> </p>
                            <li>Designed and implemented an ‘AI yoga coach’ based on skeletal joint estimation to teach a user in real-time to enter into a template yoga pose. Trained a deep learning model based on CMU’s OpenPose for skeletal joint detection.</li>
                            <li>Developed an image-processing method to estimate user pose and depth from a 2D camera stream</li>
                            <li>Built a full-stack software solution delivering real-time feedback for users to learn and perfect poses using the virtual AI
                                Yoga coach</li>
                        </ul>
                    </div>
                </div>
                <div class="timeline-item">
                    <div class="timeline-date">2018</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Graduate Researcher</h3>
                            <p class="place">University of Southern California</p>
                        </div>
                        <ul class="timeline-bullets">
                            <p><strong>Attention-Driven Brain Computer Interface</strong> </p>
                            <li>Investigated and built a viable brain computer interface based on covert shifts in attention. Being gaze-independent, this project addressed the problem of strain on user’s eyes when used over long periods of time</li>
                            <li>Demonstrated that the changes in alpha band power in the parieto-occipital region effectively indicate covert attention shifts, noting variation in the most distinguishable directions across individuals.</li>
                            <li>Achieved a 70.6% classification accuracy, showing the feasibility of a BCI using generalized linear models to detect covert attention shifts through EEG data.</li>
                            <a href="#usc_bci" class="link-to-research">
                                See related project
                            </a>
                        </ul>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-date">2017</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Undergraduate Researcher</h3>
                            <p class="place">Indian Institute of Science, Bangalore, India</p>
                        </div>
                        <ul class="timeline-bullets">
                            <p><strong>Automatic Estimation of Speaker Traits using Speech Signals </strong> </p>
                            <li>Trained a Gaussian Mixture Model (GMM) coupled with support vector regressors to estimate speaker traits such as height and age using speech signals</li>
                            <li>Achieved a mean square error of 6 cm for height and 5 years for age, aligning with then state-of-the-art technology</li>
                        </ul>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-date">2016</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Mitacs Undergraduate Research Fellow</h3>
                            <p class="place">University of Toronto and Holland Bloorview Kids Rehabilitation Hospital, Toronto, Canada</p>
                        </div>
                        <ul class="timeline-bullets">
                            <p><strong>Development of an optimal Brain Computer Interface based on Motor Imagery  </strong> </p>
                            <li>Built a BCI system based on imagined hand movement, achieving an 80% classification accuracy, offering a potential access solution for individuals with motor disabilities.</li>
                            <li>Developed an innovative algorithm using frequency domain analysis to remove real hand motion artifacts in EEG data, boosting BCI performance</li>
                            <li>Administered and managed EEG and MEG data collection protocols, including the use of wet cap electrodes and saline solution preparation.</li>
                            <li>Presented my research findings at the 10th Annual Summer Student Research Day, Holland Bloorview Kids’ Rehabilitation Hospital, Toronto, Canada</li>
                            <a href="#uoft_bci" class="link-to-research">
                                See related project
                            </a>
                        </ul>
                    </div>
                </div>

                <div class="timeline-item">
                    <div class="timeline-date">2015</div>
                    <div class="timeline-content">
                        <div class="timeline-header">
                            <h3>Raman Research Institute Visiting Student Research Fellow</h3>
                            <p class="place">Raman Research Institute, Bangalore, India</p>
                        </div>
                        <ul class="timeline-bullets">
                            <p><strong> Optimal Analog to Digital Converters for Murchinson Widefield Array of Telescopes in high frequency GHz range </strong> </p>
                            <li>Analog to Digital Converters are at the front-end of every electronic equipment. At high frequencies such as over 5-10 GHz such as in Murchinson Widefield Array of Telescopes in Australia, they become a challenging design problem </li>
                            <li>Conducted comprehensive literative review and documentation on state-of-the-art ADCs and provided recommendations for the telescopes</li>
                            <li>Researched about improving the frequency range through parallelization techniques and presented my findings at the Department meetings</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section id="talks">
            <h2>Invited Talks</h2>

            <div class="video-container">
                <iframe 
                    width="400" 
                    height="225" 
                    src="https://www.youtube.com/embed/zzLM7xN1Fts" 
                    frameborder="0" 
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                    allowfullscreen>
                </iframe>
            </div>

            <ul>
                <li>
                    <em>"Learning to See Again: Advances and Challenges in Sight Recovery for Late-Onset of Blindness"</em>, 
                    <strong>Invited Guest Lecture Series</strong>, ACS College of Engineering, 
                    Bangalore, India, December 2024. 
                </li>
                <li>
                    <em>"From Cellular Responsiveness to Functional Vision: Optogenetic Sight Recovery"</em>, 
                    <strong>Vision Science Trainee Seminar Series </strong>, University of Washington, 
                    Seattle, USA, November 2024.
                </li>
                <li>
                    <em>"Putting Functional in Restored Vision in the Blind: Characterizing the Perceptual Performance of Optogenetic Vision"</em>, 
                    <strong>Conectome 2024 </strong>, University of Washington, 
                    Seattle, USA, May 2024.
                </li>
                <li>
                    <em>"How Good is Optogenetic Vision - Can we Restore Vision in the Blind?"</em>, 
                    <strong>52nd Annual Psychology Research Festival </strong>, University of Washington, 
                    Seattle, USA, May 2023.
                </li>
            </ul>
            
            <h2>Poster Presentations</h2>
            <ul>
                <li>
                    Mohan, V., Yucel, E.I., Boynton, G., Fine, I. <em>"Characterizing Perceptual Performance of Optogenetic Vision: Moving
                        from Cellular Responsiveness to Functional Vision”.</em>, 
                    <strong>The Eye and the Chip 13th World Research Congress on Artificial
                        Vision </strong>, Southfield, MI, October 2023.
                        <br> <a href="pdfs/eyenthechip.pdf" target="_blank">[Click to view the full poster]</a>
                </li>
                <li>
                    Mohan, V., Emami, Z., Chau, T.  <em>"Development of an optimal Brain Computer Interface based on Motor Imagery us-
                        ing Electroencephalography".</em>, 
                    <strong>10th annual Anne & David Ward Family Summer Student Research Day </strong>, Toronto, July 2016.
                    <br> <a href="pdfs/uoft_bcis.pdf" target="_blank">[Click to view the full poster]</a>
                </li>
            </ul>
        </section>




        <section id="cv">
            <h2>Curriculum Vitae</h2>
        <div class="social-icons"></div>
            <a href="pdfs/Resume_VaishnaviBMohan.pdf" target="_blank" class="resume-link">
                <i class="fas fa-file-alt"></i> One page Resume
            </a>
            <br>
            <a href="pdfs/CV_VaishnaviBMohan.pdf" target="_blank">
                <i class="fas fa-file-pdf"></i> The whole spiel (long format) CV
            </a>
        </div>
        </section>
        
        
        <section id="contact">
            <h2>Contact</h2>
            <div class="social-icons">
                <a href="mailto:vaishnavibm95@gmail.com" target="_blank" class="contact-icon">
                    <i class="fas fa-envelope"></i> vaishnavibm95@gmail.com
                </a>
                <a href="mailto:vbmohan@uw.edu" target="_blank" class="contact-icon">
                    <i class="fas fa-envelope"></i> vbmohan@uw.edu
                </a>
                <a href="https://www.linkedin.com/in/vaishnavi-b-mohan1995/" target="_blank" class="linkedin-icon">
                    <i class="fab fa-linkedin"></i> LinkedIn
                </a>
                <a href="https://github.com/Vaishnavi-B-Mohan" target="_blank" class="github-icon">
                    <i class="fab fa-github"></i> GitHub
                </a>
            </div>
            
        </section>
    </main>

    <footer>
        <p>&copy; 2025 Vaishnavi B Mohan. All Rights Reserved.</p>
    </footer>
</body>
</html>
